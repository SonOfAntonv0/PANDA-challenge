{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Description\nWelcome to Prostate cANcer graDe Assessment (PANDA) Challenge. The task of this competition is classification of images with cancer tissue. The main challenge of this task is dealing with images of extremely high resolution and large areas of empty space. So, effective locating the areas of concern and zooming them in would be the key to reach high LB score.\n\nIn this competition I found a number of public kernels performing straightforward rescaling the input images to square. However, for this particular data such an approach is not very efficient because the aspect ratio and size of provided images are not consistent and vary in a wide range. As a result, the input images are deformed to large extend in a not consistent manner uppon rescaling that limits the ability of the model to learn. Moreover, the input consists of large empty areas leading to inefficient use of GPU memory and GPU time.\n\nIn this kernel I propose an alternative approach based on **Concatenate Tile pooling**. Instead of passing an entire image as an input, N tiles are selected from each image based on the number of tissue pixels (see [this kernel](https://www.kaggle.com/iafoss/panda-16x128x128-tiles) for description of data preparation and the [corresponding dataset](https://www.kaggle.com/iafoss/panda-16x128x128-tiles-data)) and passed independently through the convolutional part. The outputs of the convolutional part is concatenated in a large single map for each image preceding pooling and FC head (see image below). Since any spatial information is eliminated by the pooling layer, the Concat Tile pooling approach is nearly identical to passing an entire image through the convolutional part, excluding predictions for nearly empty regions, which do not contribute to the final prediction, and shuffle the remaining outputs into a square map of smaller size. Below I provide just a basic kernel only illustrating this approach. In my first trial I got 0.76 LB score, top 2 at the moment, and I believe it could be easily boosted to 0.80+. I hope you would enjoy my kernel, and please also check my submission kernel implementing the tile based approach.\n\n![](https://i.ibb.co/hF6LRVm/TILE.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q efficientnet_pytorch   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom sklearn.metrics  import confusion_matrix\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import SaveModelCallback,ReduceLROnPlateauCallback\nimport os\nimport numpy as np\nimport matplotlib.pyplot as py\nfrom tqdm import tqdm\nfrom sklearn.metrics import cohen_kappa_score\nfrom radam import *\nfrom csvlogger import *\nfrom mish_activation import *\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings(\"ignore\")\nfrom efficientnet_pytorch import EfficientNet\nfastai.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input/panda-dataset-medium-32-256-256/train_medium_32_256_256');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sz = 256\nbs = 2\nnfolds = 4\nSEED = 2020\nN = 32 #number of tiles per image\nTRAIN = '../input/panda-dataset-medium-32-256-256/train_medium_32_256_256/'\nLABELS = '../input/prostate-cancer-grade-assessment/train.csv'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"Use stratified KFold split."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(LABELS).set_index('image_id')\nfiles = set([p[:32] for p in os.listdir(TRAIN)])\ndf = df.loc[files]\ndf = df.reset_index()\nsplits = StratifiedKFold(n_splits=nfolds, random_state=SEED, shuffle=True)\nsplits = list(splits.split(df,df.isup_grade))\nfolds_splits = np.zeros(len(df)).astype(np.int)\nfor i in range(nfolds): folds_splits[splits[i][1]] = i\ndf['split'] = folds_splits\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['split'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Binning Labels\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bin(x):\n    n=np.zeros(6).astype(np.float32)\n    n[:x]=1\n    return n\n\ndf['ord_label']=df['isup_grade'].apply(bin)\n\np1=pd.DataFrame(df['ord_label'].values)\n\nl=[]\nfor i in range(p1.shape[0]):\n    l.extend(p1.iloc[i,:].values[0])\n\nn=np.array(l).reshape(p1.shape[0],6)\n\np2=pd.DataFrame(n)\n\ndf=pd.concat([df,p2],axis=1)\n\ndf.columns=['image_id', 'data_provider',    'isup_grade', 'gleason_score',\n               'split',     'ord_label','ord_0','ord_1','ord_2','ord_3','ord_4','ord_5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check [this kernel](https://www.kaggle.com/iafoss/panda-16x128x128-tiles) for image stats. Since I use zero padding and background corresponds to 255, I invert images as 255-img when load them. Therefore, the mean value is computed as '1 - val'."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = torch.tensor([1.0-0.90949707, 1.0-0.8188697, 1.0-0.87795304])\nstd = torch.tensor([0.36357649, 0.49984502, 0.40477625])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code below creates ImageItemList capable of loading multiple tiles of an image. It is specific for fast.ai, and pure Pytorch code would be much simpler."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def open_image(fn:PathOrStr, div:bool=True, convert_mode:str='RGB', cls:type=Image,\n        after_open:Callable=None)->Image:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning) # EXIF warning from TiffPlugin\n        x = PIL.Image.open(fn).convert(convert_mode)\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    if div: x.div_(255)\n    return cls(1.0-x) #invert image for zero padding\n\nclass MImage(ItemBase):\n    def __init__(self, imgs):\n        self.obj, self.data = \\\n          (imgs), [(imgs[i].data - mean[...,None,None])/std[...,None,None] for i in range(len(imgs))]\n    \n    def apply_tfms(self, tfms,*args, **kwargs):\n        for i in range(len(self.obj)):\n            self.obj[i] = self.obj[i].apply_tfms(tfms, *args, **kwargs)\n            self.data[i] = (self.obj[i].data - mean[...,None,None])/std[...,None,None]\n        return self\n    \n    def __repr__(self): return f'{self.__class__.__name__} {img.shape for img in self.obj}'\n    def to_one(self):\n        img = torch.stack(self.data,1)\n        img = img.view(3,-1,N,sz,sz).permute(0,1,3,2,4).contiguous().view(3,-1,sz*N)\n        return Image(1.0 - (mean[...,None,None]+img*std[...,None,None]))\n\nclass MImageItemList(ImageList):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n    \n    def __len__(self)->int: return len(self.items) or 1 \n    \n    def get(self, i):\n        fn = Path(self.items[i])\n        fnames = [Path(str(fn)+'_'+str(i)+'.png')for i in range(N)]\n        imgs = [open_image(fname, convert_mode=self.convert_mode, after_open=self.after_open)\n               for fname in fnames]\n        return MImage(imgs)\n\n    def reconstruct(self, t):\n        return MImage([mean[...,None,None]+_t*std[...,None,None] for _t in t])\n    \n    def show_xys(self, xs, ys, figsize:Tuple[int,int]=(300,50), **kwargs):\n        rows = min(len(xs),8)\n        fig, axs = plt.subplots(rows,1,figsize=figsize)\n        for i, ax in enumerate(axs.flatten() if rows > 1 else [axs]):\n            xs[i].to_one().show(ax=ax, y=ys[i], **kwargs)\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(fold=0):\n    return (MImageItemList.from_df(df, path='.', folder=TRAIN, cols='image_id')\n      .split_by_idx(df.index[df.split == fold].tolist())\n      .label_from_df(cols=['isup_grade'],)\n      .transform(get_transforms(flip_vert=False,),size=sz,padding_mode='zeros')\n      .databunch(bs=bs,num_workers=4))\n\ndata = get_data(3)\ndata.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Unet \nhttps://github.com/milesial/Pytorch-UNet/blob/master/unet/"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport math\n\nclass Selayer(nn.Module):\n\n    def __init__(self, inplanes):\n        super(Selayer, self).__init__()\n        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(inplanes, int(inplanes / 16), kernel_size=1, stride=1)\n        self.conv2 = nn.Conv2d(int(inplanes / 16), inplanes, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n\n        out = self.global_avgpool(x)\n\n        out = self.conv1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        return x * out\n\n\nclass BottleneckX(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):\n        super(BottleneckX, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n\n        self.conv2 = nn.Conv2d(planes * 2, planes * 2, kernel_size=3, stride=stride,\n                               padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 2)\n\n        self.conv3 = nn.Conv2d(planes * 2, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n\n        self.selayer = Selayer(planes * 4)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out = self.selayer(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEResNeXt(nn.Module):\n\n    def __init__(self, block, layers, num_classes,cardinality=32):\n        super(SEResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        \n        nc = self.layer4[-1].selayer.conv2.weight.shape[0]\n        self.head = Head(n_classes=num_classes,in_features=nc)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \n        \n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, self.cardinality, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, self.cardinality))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, *x):\n        shape=x[0].shape\n        n_tiles=12#len(x)\n        x=torch.stack(x,1).view(-1,shape[1],shape[2],shape[3])\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        print(f'After pooling: {x.size()}')\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        print(f'After layer4: {x.size()}')\n        print(x)\n        shape1=x.size()\n        x=x.view(-1,shape1[1],n_tiles*shape1[2],shape1[2])\n        x1 = self.head(x)\n        return x1\nclass Head(nn.Module):\n    def __init__(self,n_classes,in_features):\n        super(Head,self).__init__()\n        self.head=nn.Sequential(AdaptiveConcatPool2d(),Flatten(),nn.Linear(2*in_features,n_classes))\n    def forward(self,x):\n        return self.head(x)\n\ndef se_resnext50(**kwargs):\n    \"\"\"Constructs a SE-ResNeXt-50 model.\n    Args:\n    num_classes = 1000 (default)\n    \"\"\"\n    model = SEResNeXt(block=BottleneckX, layers=[3, 4, 6, 3], num_classes=6)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1,stride=(2,2)),\n            nn.BatchNorm2d(mid_channels),\n            Mish(),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1,stride=(2,2)),\n            nn.BatchNorm2d(out_channels),\n            Mish()\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.m=EfficientNet.from_pretrained(num_classes=n_classes,model_name='efficientnet-b0')\n        nc=list(self.m.children())[-2].in_features\n        self.head1=Head(in_features=nc,n_classes=n_classes)\n    def print(self,x):\n        print(x[0].size())\n        fig,ax=py.subplots(4,3)\n        j=0\n        for ax1 in ax:\n            for ax2 in ax1:\n                print(x[j,0,:,:].detach().numpy().size())\n                ax2.imshow(x[j,0,:,:].detach().numpy())\n                j+=1\n    \n    def forward(self, *x):\n        shape=x[0].shape\n        n_tiles=len(x)\n        x=torch.stack(x,1).view(-1,shape[1],shape[2],shape[3])\n        x1 = self.m.extract_features(x)\n        shape1=x1.size()\n        x1=x1.view(-1,shape1[1],n_tiles*shape1[2],shape1[2])#shape1[2]\n        img = self.head1(x1)\n        shape1=img.size()\n        return img\nclass Head(nn.Module):\n    def __init__(self,n_classes,in_features):\n        super(Head,self).__init__()\n        self.head=nn.Sequential(AdaptiveConcatPool2d(),Flatten(),nn.Linear(2*in_features,n_classes))\n    def forward(self,x):\n        return self.head(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SaveModelCallback(TrackerCallback):\n    \"A `TrackerCallback` that saves the model when monitored quantity is best.\"\n    def __init__(self, learn:Learner, monitor:str='valid_loss', mode:str='auto',\n                 every:str='improvement', name:str='bestmodel'):\n        super().__init__(learn, monitor=monitor, mode=mode)\n        self.every,self.name = every,name\n        if self.every not in ['improvement', 'epoch']:\n            warn(f'SaveModel every {self.every} is invalid, falling back to \"improvement\".')\n            self.every = 'improvement'\n                 \n    def jump_to_epoch(self, epoch:int)->None:\n        try: \n            self.learn.load(f'{self.name}_{epoch-1}', purge=False)\n            print(f\"Loaded {self.name}_{epoch-1}\")\n        except: print(f'Model {self.name}_{epoch-1} not found.')\n\n    def on_epoch_end(self, epoch:int, **kwargs:Any)->None:\n        \"Compare the value monitored to its best score and maybe save the model.\"\n        if self.every==\"epoch\": \n            torch.save(learn.model.state_dict(),f'{self.name}_{epoch}.pth')\n        else: #every=\"improvement\"\n            current = self.get_monitor_value()\n            if current is not None and self.operator(current, self.best):\n                self.best = current\n                torch.save(learn.model.state_dict(),f'{self.name}.pth')\n\n    def on_train_end(self, **kwargs):\n        \"Load the best model.\"\n        if self.every==\"improvement\" and os.path.isfile(f'{self.name}.pth'):\n            self.model.load_state_dict(torch.load(f'{self.name}.pth'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CutMixUpCallback(LearnerCallback):\n    \"Callback that creates the mixed-up input and target.\"\n    def __init__(self, learn, alpha=0.4, cutmix_alpha=1.0,cutmix_prob=0.5):\n        super().__init__(learn)\n        self.alpha = alpha\n        self.cutmix_prob = cutmix_prob\n        self.cutmix_alpha = cutmix_alpha\n\n    def on_train_begin(self, **kwargs):\n        self.learn.loss_func = MixLoss(self.learn.loss_func)\n        #pass\n    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n        last_input=torch.stack(last_input,1)\n        last_target=last_target.view(-1,1)\n        if not train: \n            return\n\n        # Implement cutmix\n        if np.random.rand() < self.cutmix_prob:\n            B, W, H = last_input.size(0), last_input.size(1), last_input.size(2)\n            indices = torch.randperm(B)\n            Lam = []\n            last_input_ = last_input.clone()\n            for i in range(B):\n                lam = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n                cut_rat = np.sqrt(1. - lam)\n                cut_w = np.int(W * cut_rat)\n                cut_h = np.int(H * cut_rat)\n\n                # uniform\n                cx = np.random.randint(W)\n                cy = np.random.randint(H)\n\n                bbx1 = np.clip(cx - cut_w // 2, 0, W)\n                bby1 = np.clip(cy - cut_h // 2, 0, H)\n                bbx2 = np.clip(cx + cut_w // 2, 0, W)\n                bby2 = np.clip(cy + cut_h // 2, 0, H)\n                last_input[i, bbx1:bbx2, bby1:bby2] = last_input_[indices[i], bbx1:bbx2, bby1:bby2]\n                # adjust lambda to exactly match pixel ratio\n                Lam.append(1 - (bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n            Lam = torch.tensor(Lam).cuda()\n            new_input = last_input\n            new_target = torch.cat([last_target.float(), last_target[indices].float(), Lam[:,None].float()], 1).long()\n            \n        else:\n            lambd = np.random.beta(self.alpha, self.alpha, last_target.size(0))\n            lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n            lambd = last_input.new(lambd)\n            shuffle = torch.randperm(last_target.size(0)).to(last_input.device)\n            # No stack x, compute x\n            out_shape = [lambd.size(0)] + [1 for _ in range(len(last_input.shape) - 1)]\n            new_input = (last_input * lambd.view(out_shape) + last_input[shuffle] * (1-lambd).view(out_shape))\n            # Stack y\n            new_target = torch.cat([last_target.float(), last_target[shuffle].float(), lambd[:,None].float()], 1).long()\n        new_input=new_input.permute(1,0,2,3,4)\n        return {'last_input': list(new_input), 'last_target': new_target}  \n\n    def on_train_end(self, **kwargs):\n        self.learn.loss_func = self.learn.loss_func.get_old()\n        #pass\nclass MixLoss(Module):\n    \"Adapt the loss function `crit` to go with mixup &amp; cutmix.\"\n    def __init__(self, crit, reduction='mean'):\n        super().__init__()\n        if hasattr(crit, 'reduction'): \n            self.crit = crit\n            self.old_red = crit.reduction\n            setattr(self.crit, 'reduction', 'none')\n        else: \n            self.crit = partial(crit, reduction='none')\n            self.old_crit = crit\n        self.reduction = reduction\n\n    def forward(self, output, target):\n        if len(target.shape) == 2 and target.size(-1)==3:\n            loss1, loss2 = self.crit(output,target[:,0]), self.crit(output,target[:,1])\n            d = loss1 * target[:,-1] + loss2 * (1-target[:,-1])\n        else:  \n            target=target.long()\n            d = self.crit(output, target)\n\n        if self.reduction == 'mean':    \n            return d.mean()\n        elif self.reduction == 'sum':   \n            return d.sum()\n        return d\n\n    def get_old(self):\n        if hasattr(self, 'old_crit'):  return self.old_crit\n        elif hasattr(self, 'old_red'): \n            setattr(self.crit, 'reduction', self.old_red)\n            return self.crit\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModifiedCrossEntropy(nn.Module):\n    def __init__(self):\n        super(ModifiedCrossEntropy,self).__init__()\n    def forward(self,input,target,reduction='mean'):\n        target=target.long()\n        input=input.float()\n        return loss/len(input[0])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Kappa(output,target):\n    output=output.cpu()\n    target=target.cpu()\n    output=output.sigmoid().sum(1).round()\n    target=target.sum(1)\n    print(output)\n    print(target)\n    print(cohen_kappa_score(output,target,weights='quadratic'))\n    return torch.tensor(cohen_kappa_score(output,target,weights='quadratic'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel=UNet(3,6)\nlearn = Learner(data, model, loss_func=nn.CrossEntropyLoss(), opt_func=Adam, \n                metrics=[KappaScore(weights='quadratic')],model_dir='.')\n\nlogger = CSVLogger(learn, f'log')\nlearn.clip_grad = 1.0\nlearn.split([model.head1])\n\nlearn.unfreeze()\nlearn.lr_find()\n\nlearn.recorder.plot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(7,max_lr=slice(4e-4,9e-4),\n                    callbacks=\n                    [logger,SaveModelCallback(learn,name='bestmodel',every='epoch',monitor='KappaScore'),\n                     ReduceLROnPlateauCallback(learn,),])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.export()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
